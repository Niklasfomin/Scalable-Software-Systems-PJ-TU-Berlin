// write technologies cursive	

# PJ Scalable Software Systems - Report
* Come up with another title like:
	* Cloud Benchmarking - Measuring the impact of interfering processes on cloud servers 
	* ...

## 0 Abstract

## 1 Introduction

* Begin with Cloud Service Benchmarking
* Move on to motivate Duet Benchmarking 
* Motivate by interesting hypotheses
* Mention statistical methods suitable for professional analysis
* Motivate usage of time series analysis in this project

1.[Add citations]
Cloud service benchmarking plays a crucial role in the optimization and validation of cloud-based resources, addressing several significant challenges inherent to the field. 
Research has demonstrated the value of integrating advanced suites within cloud-based CI/CD pipelines to detect crucial performance variations. To mimic real-world conditions, studies often employ virtual machines (VMs) or container technologies that are configured anew for each experiment, mitigating inconsistencies from cloud-specific anomalies or variance between instances due to networking. 
One innovative approach in this domain is Duet Benchmarking, which seeks to address these issues by orchestrating a setup where multiple Systems Under Test (SUTs) are operated concurrently on a single host machine, while the benchmark server is positioned on a distinct cloud server to secure consistent and dependable outcomes. Furthermore aims this methodology to accurately gauge the influence of a cloud server that concurrently hosts multiple applications or SUTs, shedding light on the reproducibility of benchmark outcomes. Concurrently, change point detection techniques specialize in identifying precise moments of significant performance shifts, offering an analytical edge in understanding software behavior over time. These approaches together empower a more accurate and reliable performance evaluation in cloud environments, vital for continuous software improvement and ensuring optimal operation. While VMs have been the conventional choice, the emergence of container technology offers a lighter, more resource-efficient alternative. # Put something from benchmarking paper in.
This project work serves as an initial exploration of current potentials in the cloud service benchmarking domain, whereras the goal lies in /cursive (1) Setting up a prototype for a highly automated benchmarking environment and (2) in exploring the field of statistical tools that allow for appropriate analysis of benchmarking data. For the SUT the choice is the relational database management system (RDBMS) PostgreSQL which is benchmarked by the HammerDB engine, hosted on two distinct servers on the Google Cloud Platform (GCP). 
During the benchmark execution, an additional workload is deployed on the SUT server to simulate realistic operational conditions on cloud servers. The experiment serves as a fundamental framework with the potential for scaling, aiming to contribute to the domain of application duet benchmarking in cloud ecosystems. It intends to leverage advanced virtualization technologies, notably Docker and Linux Containers (LXC), to assess how interference effects their isolated resources and efficiency. 

## 2 Related Work

* HammerDB papers
* MongoDB CPD paper
* Mention Isolation paper
* Duet Benchmarking paper
* Evaluation of different CPD Methods 

The foundation of this study builds on related work in the realm of cloud service benchmarking. Although the field itself is well-established, the concrete focus extends towards novel methodologies. The study of methods in microbenchmarking, duet benchmarking, and the use of statistical measures provides not just a context but also an inspiration to further develop and expand upon current research. The aim is to experiment with a analytical toolbox, thereby contributing to a better understanding of new frontiers in cloud service benchmarking.
[Grambow et al.] investigates the viability of using optimized microbenchmark suites for detecting application performance changes using a duet benchmarking strategy. Their study delves into the comparative effectiveness of microbenchmarks and traditional application benchmarks within CI/CD pipelines, particularly under the constraints of frequent code updates and the need for swift feedback. Their approach is backed up the research outlined in "Duet Benchmarking: Improving Measurement Accuracy in the Cloud" by [Bulej et. al]. They suggest that duet measurement is not only feasible but also advantageous for performance regression testing in cloud infrastructures, prompting further exploration into its applicability within CI/CD pipelines, especially in environments without dedicated virtual instances. 
In the realm of software performance analysis, the significance of change point detection as a method for identifying performance variations is underscored in the study of Hunter by [Fleming et. al], an open-source tool designed to detect both performance regressions and improvements within time-series data effectively. By comparing Hunter against established algorithms like PELT and DYNP using real time-series data the study demonstrates Hunter's capabilities in identifying performance shifts.
Next to methodogical approaches [Avula and Zou's] research provides an insightful comparison of the performance of the TPC-C benchmark—a well-regarded industry standard for Online Transaction Processing (OLTP) systems—across three major cloud service providers: Amazon Web Services (AWS), Microsoft Azure, and GCP. Their study leverages this benchmark to understand how these platforms handle transaction-intensive workloads by evaluating different aspects of cloud performance.


## 3 Contribution

* Outline the motivation again of this toy project and show how potential
can be used on a scale
* Mention high-level architectural setup and what it is used for

This project work aims to provide insights into performance interference of duet benchmarks at runtime for a containerized SUTs. The experimental framework offers a highly automated code base, which is adaptable not only to the chosen computational infrastructure, tool stack, and analysis methodology but is also designed for customization and scalability leading into further research possibilites.
Key to this setup is the ability to observe the particular effects that a resource-intensive application could impose on benchmarking accuracy by incorporating a fixed-timed execution of an resource exhaustive Go program. This feature simulates operational stress, and can be investigated by monitoring capabilities fetching time-series data from the virtualization hypervisor. 
Incorporating time-series analysis, particularly change point detection methods suggested by [MongoDB] research, aligns with the recognized utility of these analytical means in identifying performance inflections.

## 4 Experiment Setup

The technical specification of the experimental framework can be seen in [Table I.]. The local computer being the orchestrative machine, Terraform is used to provision the compute infrastructure. Within the Terraform script, a local executor is integrated to facilitate the invocation of an Ansible playbook, which automates the deployment of software on both the benchmark and the SUT server. 
The design of the prototype setup is meant to furnish a reproducible, automated, and comparable environment for conducting duet benchmarking experiments. Upon establishing the infrastructure one is advised to establish ssh-connections to the benchmark server — and ideally to the SUT as well—to examine logs for diagnosing any potential issues related to connectivity or otherwise. As an entry-point the benchmark server introduces an Experiment Wizard, which acts as the primary interface for initiating and concluding the experiment. This tool allows users to select and configure two possible SUTs. During the experiment, the benchmark server records and logs data from HammerDB operations. Once the benchmarking process concludes successfully, users are redirected back to the Experiment Wizards interface.
Picking up on [Avula and Zou's] and [AWS] work HammerDB is the choice as the benchmarking tool to generate database loads. HammerDB implements the TPC-C benchmark, published by the TPC for OLTP. The TPC-C specification on which TPROC-C is based implements a computer system to fulfill orders from customers to supply products from a company. The benchmarks mocks a company selling items and keeps its stock in warehouses. The test schema can be as small or large as you wish with a larger schema meaning an increased level of transactions. The system's workload comprises a diverse mix of five transaction types. These transactions are randomly selected in alignment with their respective percentage distributions.
The following [Listing] shows the HammerDB configuration with regards to target metrics for post-benchmark time-series analysis while for detailed information HammerDB's documentaiton should be compiled\footnote{\href{http://www.example.com}:  

1. ** Benchmark: TPC-C
2. ** Database: PostgreSQL
3. ** Transactional Iterations: 10.000.000	
3. ** Virtual Users: Set to the number of CPUs on the system.
4. ** Warehouses: Is multiplied by 5 with the amount of virtual users
4. ** Benchmark Duration: 20 Minutes
5. ** Timeprofile: etprof\footnote{HammerDB offers xtprof as the standard time profile to calculate MIN, MAX, AVG & the Percentiles P50, P99, P95, ratio and the standard deviation. However only cumulative values over the benchmark run are availabel and not time-series is calculated. Therefore the older etprofile was modified to print the Percentiles every 2 seconds.
6. ** Metrics: Transaction Response Times in P50% \footnote{For the analysis only the Neword Transaction Type is chosen due to having the highest occurence in the distribution with 45%.} (in milliseconds), Transaction Counts (#)

## 5 Implementation

This section provides the conceptual overview of the systems behavior from a users perspective. Figure XY shows a schematic representation of the experiment setup where Numbers 1-7 are meant for reproducing the experiment execution. 
The complete source code inlcuding benchmark data and results of this project are available on Github\footnote{\href{http://www.github.com}}

1. **Provision Infrastructure:** Terraform is employed for setting up and configuring the infrastructure necessary for benchmarking and the SUT servers aiming to establish an environment adaptable to development. This step lays the groundwork for all subsequent actions.

2. **Setup of the SUTs:** Using a Ansible playbook the setup and deployment of necessary software on the benchmark server is streamlined by installing HammerDB alongside shell scripts written in Bash for coordinating the benchmark execution. On the SUT server its specific dependencies, software, SUT-setup scripts and the Interruptor Application are installed and ready to be launched.

3. **Run HammerDB benchmark:** The Experiment Wizard initiates the benchmarking run, engaging with the SUT via ssh to execute a shell script that incorporates connection verification within the distributed setup. The Wizard then oversees the execution of the benchmark through four main stages: /cursive (1) Schema Build (2) Run Benchmark (3) Calculate results (4) delete schema.

4. **Run Interruptor App:** Following the initial benchmark stage, the experiment proceeds the activation of the interruptor application. The Interruptor is scheduled to start five minutes after its invocation and will run for five minutes before it self-deactivates. This introduces an additional layer of realism to the testing environment bringig up the assumption of causing to visible change points in the time-series data. The Go program that serves as a stress test is designed to intensely utilize CPU and memory through concurrent processes. The application leverages the Go runtime's ability to optimize CPU core usage, thus exploiting system parallelism. Its tasks, such as calculating π, matrix multiplication, and memory copying, showcase varying algorithmic complexities and computational intensities—from CPU-bound operations to memory bandwidth demands. The implementation of concurrency utilizing goroutines and synchronization demonstrates practical parallel processing and thus put system properly under load.

5. **Monitor resource consumption:** Concurrently to the invocation of the Interruptor, a monitoring script written in Bash for Docker Engine and LXC detects the current SUT after the schema is constructed providing measurements of the SUTs CPU and Memory consumption at a time-frequency adpated to the calculation of the transactional metrics (except Transaction Counts).

6. **Local analysis:** Post-benchmark, HammerDB's output along with the collected resource usage data undergoes local analysis. The Experiment Wizard enhances this process by compensating for HammerDB's limited logging capabilities, supplying functionality for filtering the relevant metrics results ready for refinement. To conclude the experiment the SUT server undergoes a cleanup process.

7. **Interpretation of results:** Finally, the outcomes of the benchmark are processed for time series analysis. The goal is to detect the impact of the Interruptor application at the known /cursive Start and End Time of the app. For this sake the local computer offers a prepared Jupyter notebook offering simple preprocessing for each of the three recorded metrics per SUT leading to visualization change point detection on the time-series data.

## 6 Analysis

* describe approach to analysis
* describe the desired results 
* describe the methods to compare the SUTs based on metrics 
* outline the used methods in the analysis 
* discuss the basis and intention of the analysis
* insert result table of the overall comparison and of the metrics 
tables within each SUT
* outline interpretation of the results
* come up with different reasons or explanations for the results based on technical principles

The following analysis is concerned with a basic exploration of the broad spectrum of statistical methods applied to time-series data. The main intention lays in taking the raw data followed by "trying out" and comparing the performance of different detection algorithms with different hyperparameters. While letting much room for further preprocessing of the data as well as precise paramter-tuning, the desried result should much more open an initial window how change point detection algorithms perform on raw database benchmark data overall. Therefore desired result of the analysis is to gain insight if the detected change points match the actual timestamps of the Interruptor application start and end time. 

1. ** Methodogical explanation and approach

The analysis is done using Python 3.12.2 makes use of the features implemented in the following libraries.

**Matplotlib**
**Pandas**
**Numpy**
**Ruptures**
**Seaborn**

2. ** Description of Algorithms

With a focus on univariate data, this analysis uses both the Pruned Exact Linear Time (PELT) algorithm and Dynamic Programming (Dynp) technique offered by ruptures, chosen for their alignment with the analytical requirements. 
PELT is an efficient algorithm designed to identify change points in a signal while maintaining a linear computational cost relative to the number of data points. It employs a pruning rule to discard unlikely indices, thereby optimizing the search process without compromising the ability to locate the optimal segmentation. 
The Dynp algorithm accommodates various cost functions. It precisely computes the minimum sum of costs across all potential segmentations of the signal, therefore organizing the search sequence efficiently.

4. ** Results 

The application of both PELT and Dynp to the benchmark time-series data opens up significant possibilities for evaluation, comparison and analysis. Due to this projects limited scope a generic evaluation approach is formalized in the following section.

Let P = {p1,p2,...,pn} represent the set of detected change points, where n is the total number of detected change points. Let k1 and k2 represent the known change points knownCP1 and knownCP2, respectively.
Define two indictaor functions for the detection status of k1 and k2:
[Insert formal model]

The output, detectionStatus, is then defined as: detectionStatus = I1 AND I2 Where: I1 AND I2 = 1(true) if both I1 = 1 and I2 = 1, I1 AND I2 = 0(false) otherwise.

This indicator function is then computed with the simple algorithm seen in [Figure Algorithm 1]

The result of the evaluation is given by [Table 2] and all plots are uploaded to Github\footnote{href{http://Github.com}}.

5. ** Key Observations

- mention detection rate for both algorithms
- mention that change points are displayed
- result of comparison of the sliced time series for TRT
- mention that dyp is assumed to perform better but detects nothing
- discuss hyperparamter tuning and influence on results and expected behavior

## 7 Conclusion & Outlook

- write about expection and comparison between docker and lxc and the actual results
- write about possible reasons for the behavior
- mention potential in using more extensive statistical analysis means, more algorithms
- more comparisons between the results within each SUT
-  

## 8 Future Work / Outlook

* Mention how experimental setup can be extended for larger scale analysis
* Mention potential impact some future work would have to extend different SUTs
* disucss potential to extend the existing orchestration
* discuss potential to use different benchmark engine with more fine granular 
* More SUTs
understandin of what the engine does in order to have best possible data for analysis
* discuss potential in more extensive data anlysis focusing on possible techniques
like multivariate analysis, hyperparameter-tuning and usage of more complex 
detection models or a more extensive research on comparing different ones

In conclusion, the exploration and application of change point detection methodologies within duet benchmarking environments exhibit promising potential for enhancing performance regression analysis across diverse software versions. The integration of tools like Hunter and the deployment of algorithms such as E-Divisive Means, PELT, and DYNP within continuous integration systems have demonstrated substantial efficacy in pinpointing performance variations amidst noisy data settings. This automated approach not only streamlines the detection process but also minimizes the reliance on manual inspection, thereby augmenting both the precision and efficiency of performance evaluations.
Moreover, the duet measurement technique underscores the significance of external interference and resource sharing in cloud-based workload evaluations, advocating for nuanced performance regression testing across both virtual and bare-metal infrastructures. It also invites further investigation into the utility of these methods within CI/CD pipelines lacking dedicated instances.
Our analyses suggest that the optimization and meticulous monitoring of virtual machines, containers, and their corresponding hypervisor technologies can significantly influence performance outcomes. Therefore, future endeavors should concentrate on expanding the repertoire of change point detection algorithms, delving deeper into benchmarking dynamics, and elucidating potential system behaviors. Additionally, enhancing SUT monitoring would enable a more comprehensive harnessing of these methodologies' potential.
By facilitating a robust comparison of diverse system under test (SUT) configurations, informed by advanced analytical techniques and precise change point detection, we can better comprehend and optimize the multifaceted nature of cloud computing environments. Such endeavors will not only refine our methodological toolkit but also bolster our understanding of virtualization's nuanced impacts, ultimately fostering more resilient and efficient cloud-based solutions.

