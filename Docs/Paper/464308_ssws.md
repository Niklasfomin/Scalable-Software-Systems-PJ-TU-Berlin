// write technologies cursive	

# PJ Scalable Software Systems - Report
* Come up with another title like:
	* Cloud Benchmarking - Measuring the impact of interfering processes on cloud servers 
	* ...

## 0 Abstract

## 1 Introduction (TODO also mention cloud service benchmarking book, warum datenbank nennen)

* Begin with Cloud Service Benchmarking
* Move on to motivate Duet Benchmarking 
* Motivate by interesting hypotheses
* Mention statistical methods suitable for professional analysis
* Motivate usage of time series analysis in this project

This project report explores the domain of cloud service benchmarking while focusing on evaluating a novel approach called duet benchmarking.
Cloud Service Benchmarking is important because blablabla and the following problems arise in this. Duet Benchmarking targets at mitigating these issues by running multiple SUTs on a single machine while the benchmark-server resides on a different cloud server. Therefore the target is to find a mean that can measure the impact of a cloud server hosting multiple applications or SUTs at the same time on the reproduceability of benchmark runs targetting at the implementation of this cloud service benchmarking strategy in CI/CD pipelines. This work relies on statistical measurements of benchmarking metrics produced by 3 SUTs. Each SUT is running in its own isolated environments on the host-VM. To mock the servers exhaustion in realistic produciton environments a resource-intensive go program is timed to execute and stop at predefined timestamps during the benchmark run. As nowadays cloud computing environments make heavy usage of virutalization technologies just like vms or containers it is of adequate interest to statistically analyse potential impacts other resource-intensive processes running on the host-vm might have on the accuracy and reproduceability of application benchmarks in the cloud.

[Benchmarking and Performance]
Cloud computing has rapidly emerged as a vital resource, particularly for the scientific community, offering flexible, on-demand access to computational infrastructure.
[Initial Experiments]
The growing emphasis on agile development practices necessitates continuous and comprehensive software testing within CI/CD frameworks, extending beyond functional correctness to include performance consistency. The dichotomy between the thoroughness of performance tests and the constraints of rapid development cycles presents a significant challenge, as longer tests provide accuracy but are resource-intensive, while shorter tests risk unreliability. To address these issues, methods like selective testing and parallelization have been introduced, yet they do not fully overcome the limitations of testing infrastructure. The potential of cloud environments for performance testing raises questions due to their inherent variability and potential for noisy data. Our study introduces a 'duet measurement' approach, enhancing accuracy in cloud-based performance testing, validated across various environments and specifically applied to the Graal compiler.
In performance testing, particularly when assessing variations between software versions, a standard method involves employing benchmark workloads to evaluate and juxtapose the execution times of both versions, using statistical hypothesis testing to account for inherent data variability. The sources of this variability impact observed execution times at various levels, necessitating a comprehensive approach in performance testing to ensure significant factors are represented. By executing benchmarks repeatedly, variations due to system scheduling, cache behaviors, or background activities are captured. Additional runs across diverse instances help document fluctuations caused by system-level changes or runtime environment adjustments, like JIT compilation or garbage collection, that aren't consistent across different runs.
The traditional practice involves running benchmarks on dedicated hardware, minimizing external interferences such as power management features, ensuring measurement purity. However, the cloud environment, characterized by its abstracted virtual resources, introduces a layer of unpredictability due to the potential variability in underlying physical hardware and neighbor workloads sharing the same physical resources. This variance challenges the comparability of results across different virtual instances and even within the same instance over time, complicating the use of cloud-based platforms for performance benchmarks that demand a stable and controlled environment.
Thus, to leverage cloud infrastructures effectively for performance testing, it's crucial to adapt methodologies that acknowledge and mitigate these cloud-specific variables. This adaptation is essential to produce reliable and meaningful performance comparisons, facilitating accurate detection of performance shifts between software iterations within the inherently variable cloud context.

[Isolation and Sharing in the Cloud]
Cloud stacks must isolate application components, while
permitting efficient data sharing between components deployed
on the same physical host. Traditionally, the MMU
enforces isolation and permits sharing at page granularity.
MMU approaches, however, lead to cloud stacks with large
TCBs in kernel space, and page granularity requires inefficient
OS interfaces for data sharing. Forthcoming CPUs with hardware
support for memory capabilities offer new opportunities
to implement isolation and sharing at a finer granularity.
We describe cVMs, a new VM-like abstraction that uses
memory capabilities to isolate application components while
supporting efficient data sharing, all without mandating application
code to be capability-aware. cVMs share a single
virtual address space safely, each having only capabilities to
access its own memory. A cVMmay include a library OS, thus
minimizing its dependency on the cloud environment. cVMs
efficiently exchange data through two capability-based primitives
assisted by a small trusted monitor: (i) an asynchronous
read/write interface to buffers shared between cVMs; and
(ii) a call interface to transfer control between cVMs. Using
these two primitives, we build more expressive mechanisms
for efficient cross-cVM communication. Our prototype implementation
using CHERI RISC-V capabilities shows that
cVMs isolate services (Redis and Python) with low overhead
while improving data sharing.
1 Introduction
Cloud environments require application compartmentalization.
Today, isolation between application components is enforced
by virtual machines (VMs) and containers
either separately or in combination. Yet, current
applications push the limits of these mechanisms in terms
of performance and security: when application components
communicate heavily with each other, VMs and containers
add substantial overheads, even when they are co-located to
improve communication performance; furthermore, the implementation
of the isolation mechanisms may also rely on a
large trusted computing base (TCB).
VMs provide strong isolation through a relatively narrow
hardware interface. Since a guest VM has its own OS kernel,
its TCB can be reduced to a relatively small hypervisor, which
multiplexes VM access to the hardware Efficient inter-
VM data sharing, however, is challenging to achieve due to
performance and page granularity trade-offs
In contrast, containers isolate processes into groups
and provide faster inter-process communication (IPC) primitives,
including pipes, shared memory, and sockets. Similar
to VMs, they face problems of page-level sharing granularity
and overheads due to frequent user/kernel transitions. Their
richer IPC primitives for data sharing come at the cost of a
larger TCB—a shared OS kernel implements both namespace
isolation between process groups and complex IPC primitives,
increasing the likelihood of security vulnerabilities.
Existing cloud stacks thus face a fundamental tension when
application components are compartmentalized but must communicate.
They must either copy data or modify page tables,
both of which are expensive operations that involve a privileged
intermediary, e.g., a hypervisor or OS kernel, and lead
to coarse-grained interfaces designed around page granularity.
In this work, we explore a different approach to designing
a cloud stack that isolates application components, while supporting
efficient sharing.We ask the question “if the hardware
supported dynamic, low-overhead sharing of arbitrary-sized
memory regions between otherwise isolated regions, how
would this impact the cloud stack design?” We exploit hardware
support for memory capabilities which impose
flexible bounds on all memory accesses, allowing components
to be isolated without page table modifications or adherence
to page boundaries. This offers a new opportunity to design
memory sharing primitives between isolated compartments
with zero-copy semantics.

## 2 Related Work

* Maybe only two papers
	* HammerDB papers
	* MongoDB CPD paper
	* Mention Isolation paper
	* Duet Benchmarking paper
	* Evaluation of different CPD Methods 
	
Based on the papers by X and Y hammerDB is choosen as the benchmark engine for this research. hammerdb proves to be useful because it delivers workload capabilities that are realistic and complex enough for meaningful experiments.

This project is also heavily motivated by the Application benchmark paper and the 2 Duet Benchmarking papers by so and so because the explore this relatively novel field of research. 
The 2 duet benchmarking papers explore the field and give the main background for the motivation why this technique might be used and how good it proves to be in practice.

As hammerDB produces values measured over time the usage of time-series-analysis makes the most sense. The mongoDB paper suggests a specific statistical method to analyze benchmark time-series so it makes sense for this project to adapt and explore the realm of change point detection methods and make some choice for the analysis approach to be used.

## 3 Contribution

* Outline the motivation again of this toy project and show how potential
can be used on a scale
* Mention high-level architectural setup and what it is used for

This poject contributes to the field of cloud service benchmarking by delivering a statistical analysis of the impact that interfering processes in different isolation environments on a cloud have on application benchmarks. The experimental setup provides a highly automised code-base that does not only rely on the choosen compute infrastructure and tool-stack and even analysis methodology but can be easily modified, customised and extended for large-scale exploration of this promising domain of duet benchmarking in production environments. This project provides a straight forward automised creation of different SUTs as well as orchestrating benchmark runs executed at one SUT at time. To measure the specific impact an exhaustive resource consuming application might have on benchmark executions the setup provides a fixed-timed execution of an interruptive go program that can be activated or not and provides custom scripts that gathers resource monitoring information from the virutalization hypervisor for the time of the actual benchmark being executed. The resulting data can be downloaded to the localhost of the scientist for a simple, yet effective streamlined data analysis approach implemented in python and organized in a jupyter notebook where as the structure can be easily extended or modified as needed. The current implementation is however focused on working with both raw log data from hammerDB as well as common csv files.
The project results offers both as a guidance with mathematical foundation on what to expect by using duet benchmarking on applications and gives orientation on implementing this strategy on a larger scale. Additinially it provides a simple extensible framework for application benchmarking in an organized and simple, timeable manner that streamlines the process of realistic benchmarking and can be used for furhter research.

## 4 Experiment Setup

* Mention Technical Assumptions and draw borders what is covered and what not
* Inlcude picture of architecture (simplified but based on git-draw-sketch)
* Describe how components in the architecture are intended to work together
* Describe from users perspective how things are done and why
* Mention technical components, why they are used and why this is benefitial
* Describe Experiment Flow timewise 
* Describe how experiment ends and how to bridge to get to analysis

The experimental setup is build upon the google cloud platform and uses the following configuration of servers. The local machine serves as the single-point of orchestration by providing the compute infrastructure using terraform. Inside the main terraform script a local-executor is used to embed the execution of a ansible playbook that automises the software rollout on both the benchmark server and the SUT-server. The following table gives an overview on the used software-packages for the experimental setup.
As seen is figure XY the experimental setup aims at the following g.als: providing an reproducable, automised and comparable setup for a duet-benchmarking experiment. After the experiment infrastructure is present, the scientist is supposed to connect to the benchmark-server (and preferably also to the SUT) to inspect logs to troubleshoot potential connectivity or other errors. The benchmark server provides an Experiment wizard that servers as the entry and exit point for the complete experiment. Via this component the user has the choice to setup one of three SUTs at a time. In the following the benchmark run will be executed, the next section provides insights on how to configure the system behavior and what paramters to tune. The benchmark server logs the hammerDB. After the benchmark run has successfully terminated, the user is again found within the experiment wizard. 
[HammerDB Docu]
A transactional or OLTP (online transaction processing) workload is a workload typically identified by a database receiving both requests for data and multiple changes to this data from a number of users over time where these modifications are called transactions. Each database transaction has a defined beginning point, manipulates and modifies the data within the database and either commits the changes or rollbacks the changes to the starting point. A database must adhere to the ACID (Atomicity, Consistency, Isolation, Durability) properties to ensure that the database remains consistent whilst processing transactions. Database systems that process transactional workloads are inherently complex in order to manage the user sessions access to same data at the same time, processing the transactions in isolation whilst keeping the database consistent and recoverable. People will typically interact with OLTP systems on a regular basis with examples such as an online grocery ordering and delivery system or an airline reservation system. Performance and scalability are essential properties of systems designed to process transactional workloads. The TPC-C benchmark is a benchmark designed by the TPC to measure the performance of the software and hardware of a relational database system to process these workloads.
Designing and implementing a database benchmark is a significant challenge. Many performance tests and tools experience difficulties in comparing system performance especially in the area of scalability, the ability of a test conducted on a certain system and schema size to be comparable with a test on a larger scale system. When system vendors wish to publish validated benchmark information about database performance they have needed to access sophisticated test specifications and the TPC is the industry body most widely recognized for defining benchmarks. TPC specifications are the only published benchmarks in the database industry recognized by all of the leading database vendors. TPC-C is the benchmark published by the TPC for Online Transaction Processing and you can view the published TPC-C results at the TPC website. The HammerDB TPROC-C workload is intentionally not fully optimized and not biased towards any particular database implementation or system hardware, being open source you are free to inspect all of the HammerDB source code and to submit pull requests to update or enhance the workloads. The intent is to provide an out-of-the-box type experience when testing a database without requiring complex configurations or additional third-party software in addition to both HammerDB and the database you are testing. HammerDB can be run on any environment from a simple laptop based express type database install right through to 8, 16 and 32 CPU socket servers and clusters. The crucial element is to reiterate the point made in the previous section that the HammerDB workloads are designed to be reliable, scalable and tested to produce accurate, repeatable and consistent results. In other words HammerDB is designed to measure relative as opposed to absolute database performance between systems. What this means is if you run a test against one particular configuration of hardware and software and re-run the same test against exactly the same configuration you will get exactly the same result within the bounds of the random selection of transactions which will typically be within 1%. Any differences between results are directly as a result of changes you have made to the configuration (or management overhead of your system such as database checkpoints or user/administrator error). Testing has proven that HammerDB tests re-run multiple times unattended (see the autopilot feature) on the same reliable configuration produce performance profiles that will overlay each other almost identically. The Figure below illustrates an example of this consistency and shows the actual results of 2 sequences of tests run unattended one after another against one of the supported databases with the autopilot feature from 1 to 144 virtual users to test modifications to a WAL (Write Ahead Log File). In other words HammerDB will give you the same results each time, if your results vary you need to focus entirely on your database, OS and hardware configuration. he TPC-C specification on which TPROC-C is based implements a computer system to fulfil orders from customers to supply products from a company. The company sells 100,000 items and keeps its stock in warehouses. Each warehouse has 10 sales districts and each district serves 3000 customers. The customers call the company whose operators take the order, each order containing a number of items. Orders are usually satisfied from the local warehouse however a small number of items are not in stock at a particular point in time and are supplied by an alternative warehouse. It is important to note that the size of the company is not fixed and can add Warehouses and sales districts as the company grows. For this reason your test schema can be as small or large as you wish with a larger schema requiring a more powerful computer system to process the increased level of transactions. The TPROC-C schema is shown below, in particular note how the number of rows in all of the tables apart from the ITEM table which is fixed is dependent upon the number of warehouses you choose to create your schema.
The TPC Policies allow for derivations of TPC Benchmark Standards that comply with the TPC Fair Use rules. TPROC-C is the OLTP workload implemented in HammerDB derived from the TPC-C specification with modification to make running HammerDB straightforward and cost-effective on any of the supported database environments. The HammerDB TPROC-C workload is an open source workload derived from the TPC-C Benchmark Standard and as such is not comparable to published TPC-C results, as the results comply with a subset rather than the full TPC-C Benchmark Standard. The name for the HammerDB workload TPROC-C means "Transaction Processing Benchmark derived from the TPC "C" specification". For additional clarity please note that the term Warehouse in the context of TPROC-C bears no relation to a Data Warehousing workload, as you have seen TPROC-C defines a transactional based system and not a decision support (DSS) one. In addition to the computer system being used to place orders it also enables payment and delivery of orders and the ability to query the stock levels of warehouses. Consequently the workload is defined by a mix of 5 transactions selected at random according to the balance of the percentage value shown as follows:

New-order: receive a new order from a customer: 45%

Payment: update the customers balance to record a payment: 43%

Delivery: deliver orders asynchronously: 4%

Order-status: retrieve the status of customers most recent order: 4%

Stock-level: return the status of the warehouses inventory: 4%

Building upon hammerDBs capabilities the standard metrics are composed of the following measurement values accessible in different time profiles (include overview here). Therefore the main three objectives of the experiment are transaction response times, transaction counts and the systems resource consumption of the TPC-C workload. The time-interval on hammerDB's side is set to 10 seconds, while the resource monitoring queries the API of the hypervisor in 2-second intervals for new values. Here the points of interest are CPU usage in seconds and percentage, as well as the memory usage in MB/GB (make metric table).
From here the logged data can processed ready for a download to the localhost. To continue the experiment the current SUT-Server can be cleaned (include wizard options in different highlighting) followed by the setup of the next SUT. In case all experimental data is gathers the wizard can be exited and the conneciton to the server closed. The data can then be downloaded using the provided commands for the according cloud platform, in this work gcp-commands are provided.

## 5 Implementation

* Mention how  main components are done in code and how they can be extended 
or interchanged
	* Infrastructure Provisioning Terraform 
	* Software Rollout via Ansible
	* Connectivity is assumed by generated ssh-keys
	* Cover HammerDB 
		* Go into depth how HammerDB works and how it is executed.
	* Experiment Orchestration via Bash
		* Experiment Setup Wizard
			* setup of 3 different SUTS (insert table with technical overview of setup)
			* run_benchmark.sh Script
				* Orchestration via SSH 
				* integration of hammerDB .tcl calls 
			* implementation and timing of interruptor application
			* implementation and timing of resource monitor 
			* gathering of logs from execution, and how to strip and download them
	* Implementation of Analysis 
		* mention software libraries used and for what purpose
		* mention what was done and what not
		
[abstract the components by their functionality]
For the implementation we focus on a conceptual overview over the main components of this experiment setup.
Infrastructure and Software provisioning:
Terraform is used to create the benchmark and SUT-server, while configuring a development-only environment and locally executing (inlcude info about ansible local executor) an ansible playbook where the configuration and software package rollout for both servers in configured in. While the playbook is flexible in extension it currently places most importantly hammerDB and the necessary shell scripts for benchmark orchestration on the server while on the SUT-Server dependencies, SUT-Software and again orchestration scripts plus the exhaustor application are installed.
To achieve a time-consistent setup that can be steered form only one server the experiment wizard invokes the SUT-setup via ssh and then runs another shell-script that encapsulates some verification logic to mitigate troubleshooting in the cloud environment and then executes the three stages of the benchmark run via hammerDBcli tcl script invocation. hammerDB first builds the database schema on the SUT and lets the user know when its done. Next the user chooses to run the benchmark with or without the interruptor-application is timed to start 5 minutes after it gets invoked (right after the users choice and before the execution of the actual transactional workload). Because the schema build already produces load on the SUT but this load is not interesting for us simultaneously to the invoke of the interruptor the resource-monitor script is invoked. This script detects the running SUT and begins to measure resource metrics as define in the previous section. The hammerDB benchmarke execution consists of 4 consecutive steps. Build Schema, Run benchmark and Delete Schema and calculation of the results. One can influence the calculation of the metrics or the frequency. However almost the complete implementation of hammerdb is done in tcl which is not very common. Because hammerDBs limited logging capability the experiment wizard logs the benchmark output itself and provides another shell script to parse and strip the results accordingly. The cleaning of the SUT server is again done by a shell script that cleans either the docker container, the linux container or the qemu-vm.
To make data analysis more straight forward we choose jupter notebook with python kernel soandso as the tool of analysis with standard software packages ....
Each SUT hold 3 directories with the respective data per metric that can then be analysed according to different goals. For raw log data downloaded from the benchmark server the data is firstly formatted into csv and then preprocessed into python pandas dataframes as a convenient data structure. Each metric holds visualizations of its time-series where 2 lines are inlcuded that represent the 2 timestamps where the interruptor program started and terminated. Additionally and only for visual inspection the time-series are plotted on different scales and are currently let in plain format, leaving room fur further analysis.

## 6 Analysis

* describe approach to analysis
* describe the desired results 
* describe the methods to compare the SUTs based on metrics 
* outline the used methods in the analysis 
* discuss the basis and intention of the analysis
* insert result table of the overall comparison and of the metrics 
tables within each SUT
	* outline interpretation of the results
	* come up with different reasons or explanations for the results 
	based on technical principles

## 7 Conclusion

* Summarize what was done, why and how
* Sum up the results and their interpretation
* Sum up potential reasons

## 8 Future Work / Outlook

* Mention how experimental setup can be extended for larger scale analysis
* Mention potential impact some future work would have to extend different SUTs
* disucss potential to extend the existing orchestration
* discuss potential to use different benchmark engine with more fine granular 
understandin of what the engine does in order to have best possible data for analysis
* discuss potential in more extensive data anlysis focusing on possible techniques
like multivariate analysis, hyperparameter-tuning and usage of more complex 
detection models or a more extensive research on comparing different ones


